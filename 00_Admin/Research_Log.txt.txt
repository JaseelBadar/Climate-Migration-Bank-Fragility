======================================================================
RESEARCH LOG - Climate Change, Migration, and Bank Fragility
======================================================================
Project: Climate-induced migration effects on banking sector stability
Period: December 2025 - Present
Author: Jaseel Badar
Location: E:\Climate-Migration-Bank-Fragility\
======================================================================

======================================================================
2025-12-30 to 2025-12-31: Phase 1 - Project Initialization & Environment Setup
======================================================================
Goal: Set up computational environment and project structure

Environment Setup:
- Created conda environment: research_env
- Installed core packages: pandas, geopandas, rasterio, matplotlib, statsmodels
- Python 3.10.19 on Windows 11
- VS Code configured as primary IDE

Project Structure Created:
- 00_Admin/ (documentation, logs)
- 01_Data_Raw/ (original data, never modified)
- 02_Data_Intermediate/ (intermediate processing outputs)
- 03_Data_Clean/ (cleaned datasets for analysis)
- 04_Code/ (all Python scripts)
- 05_Outputs/ (regression outputs, figures)
- 06_Drafts/ (paper drafts)

GitHub Repository:
- Initialized local Git repository
- Created .gitignore (excludes large files / prevents repo bloat)
- First commit: "Initial project structure"

Key Decisions:
- Manual data download approach (API attempts for VIIRS failed)
- Focus on 2015-2024 period (10-year panel)

Status: Environment ready, awaiting data download
Next: Download RBI, EM-DAT, VIIRS datasets

======================================================================
2026-01-02 (Thu): Phase 2 - Data Acquisition (Day 1)
======================================================================
Goal: Download all three core datasets

Data Sources Acquired:

1. RBI District Banking Statistics (BSR-2)
   Location: 01_Data_Raw/RBI_Bank_Data/
   Files downloaded:
   - RBI_Deposits_2004_2017.xlsx (historical baseline)
   - RBI_Deposits_2017_2022.xlsx (5-year mid-period)
   - RBI_Deposits_2023_2024.xlsx (recent data)
   Source: Reserve Bank of India official portal
   Coverage: All districts, quarterly snapshots
   Variables: Deposits by population group (Rural/Semi-urban/Urban/Metro)

2. EM-DAT Disaster Database
   Location: 01_Data_Raw/EMDAT_Disasters/
   File: public_emdat_custom_request_2026-01-02_c149ea93-8fbf-4f6e-a8f6-3b41cc622ed0.xlsx
   Source: Centre for Research on Epidemiology of Disasters (CRED)
   Filters applied: Country=India, Disaster Type=Flood, Period=2015-2024
   Events: 70 flood events (later corrected to 69 after inspection)
   Variables: Location, dates, deaths, affected population, damage estimates

3. VIIRS Nighttime Lights (Test Tile)
   Location: 01_Data_Raw/VIIRS_NightLights/
   Test file: SVDNB_npp_20230101-20230131_75N060E_vcmcfg_v10_c202302080600
   Files extracted: .avg_rade9h.tif (avg radiance), .cvg.tif, .cf_cvg.tif
   Source: Colorado School of Mines Earth Observation Group
   Tile: 75N060E (covers all of India)
   Period tested: January 2023 (1 month)
   Note: Full download (120 months, 2015-2024) deferred to Phase 3d

Key Decisions:
- VIIRS Microsoft Planetary Computer API failed (returned 0 items)
- Pivoted to manual download from Colorado School of Mines
- Downloaded single test tile to validate format before bulk download
- RBI files span different periods, will require concatenation

Commits: 3 (data folder structure + download documentation)
Status: Phase 2 complete
Next: Phase 3a literature acquisition + Phase 3c data inspection (merge feasibility)

======================================================================
2026-01-05 (Sun): Phase 3a - Literature Acquisition
======================================================================
Goal: Build a defensible novelty baseline and assemble the core bibliography

Actions Completed:
- Searched Google Scholar, SSRN, NBER using targeted keywords
- Used Harvard HOLLIS to bypass paywalls for locked papers
- Downloaded ~15-18 papers via Zotero Chrome Connector
- Organized PDFs in 00_Admin/Literature_PDFs folder
- Fixed Zotero metadata (missing authors/dates)
- Created LiteratureTracker.xlsx with table structure

Status: Papers acquired. Ready for gap analysis reading/annotation.
Next: Phase 3b - Fill LiteratureTracker.xlsx using abstracts/intros/conclusions

======================================================================
2026-01-06 (Mon): Phase 3b - Gap Analysis
======================================================================
Goal: Confirm novelty and define “what the literature missed” in a structured way

Actions Completed:
- Read abstracts / key intro paragraphs / conclusions for the collected paper set (~15-18 papers)
- Filled LiteratureTracker.xlsx with multi-dimensional gap analysis:
  - Geographic gaps (India district-level evidence scarce / inconsistent)
  - Methodological gaps (satellite lights often used for activity proxies; migration mechanism under-tested)
  - Temporal gaps (many designs not aligned to disaster timing / dynamic panels)
  - Mechanistic gaps (banking stability often framed via credit risk; deposit/liquidity channel underemphasized)
- Confirmed novelty positioning: No paper in the reviewed set establishes the full chain:
  Climate shock → (migration proxied by VIIRS) → district-level deposit stress in India

Status: Literature phase complete. Novelty defense ready.
Next: Phase 3c - Formalize hypotheses/codebook and begin data inspection scripts

======================================================================
2026-01-07 (Tue): Phase 3c Day 0 - Conceptual Work (Mobile Only)
======================================================================
Goal: Maintain momentum and lock “definitions + hypotheses” before coding

Actions Completed:
- Laptop unavailable - executed mobile research tasks to maintain momentum
- Created Variables_Codebook_v1.md:
  - Defined variable families (outcomes, treatments, controls, networks, panel structure)
  - Defined transformations to be used in code (e.g., log changes, quarterly aggregation targets)
- Created Hypotheses_Formal_v1.md:
  - H1: Flood events → Decline in nighttime light intensity (VIIRS)
  - H2: Nighttime light decline → Decline in formal bank deposits
  - H3: Deposit decline → Increased reliance on shadow banking (timing analysis)
  - H4: Banking stress exhibits spatial contagion through district networks
- Both documents specify measurement intentions (time aggregation, geographic unit, sign expectations)

Status: Codebook + hypotheses locked. Ready to implement inspection scripts on laptop.
Next: Phase 3c Day 1 (Jan 8) - RBI/EM-DAT/VIIRS inspection scripts + merge feasibility validation

======================================================================
2026-01-08 (Wed) 19:00-21:30 PM IST: Phase 3c Day 1 - Data Inspection
======================================================================
Goal: Validate merge feasibility across 3 datasets (RBI, EM-DAT, VIIRS)

Scripts Created:
1. 02_inspect_rbi.py: RBI deposit structure inspection
   - 762 unique districts (UPPERCASE, hyphenated names)
   - 11 quarters (2023Q1-2025Q2), 4 population groups per district
   - Deposit columns at indices 7,10,13,16,19,22,25,28,31,34,37
   - Aggregation required: GROUP BY district, SUM across population groups

2. 03_inspect_emdat.py: EM-DAT flood event inspection
   - 69 flood events (2015-2024)
   - 35 events (50.7%) have district-level Admin Units data → 147 districts
   - 34 events (49.3%) have empty Admin Units (requires Location text parsing)
   - Severity measure: No. Affected (73.9% coverage)
   - Date structure: Complete, convertible to quarters

3. 04_inspect_viirs.py: VIIRS nighttime lights validation
   - Test tile (Jan 2023, 75N060E): 1.93 GB GeoTIFF, 28800×18000 pixels
   - India fully covered (60-180°E, 0-75°N extent)
   - Radiance range: 0-57.24 nW/cm²/sr (mean 0.87, median 0.50)
   - Resolution: 15 arc-second (~463m at equator)
   - Data validity: CONFIRMED

Key Findings:
- RBI: 762 districts ready for merge (standardized naming)
- EM-DAT: Geographic precision heterogeneous (50% district-level, 50% state-level)
- VIIRS: Production-ready, H1/H2 testable with district aggregation
- Expected treatment/control split: ~150 flooded districts, ~600 non-flooded
- Update (Jan 9): Admin Units are present for 57/69 events once parsed correctly (adm2_name districts + adm1_name states); only 12/69 require Location parsing.

Next Session (Phase 3c Day 2 - Jan 9):
- Parse EM-DAT Location text for missing 34 events
- Build district name harmonization crosswalk (fuzzy matching)
- Test VIIRS spatial subsetting (extract India bounding box)

Commits: 4 scripts (01_download_viirs placeholder, 02-04 inspection scripts)
Status: Phase 3c Day 1 complete

======================================================================
2026-01-09 (Fri) 17:45-18:40 PM IST: Phase 3c Day 2 - District Boundaries + EM-DAT Parsing
======================================================================
Goal: Begin district harmonization by locking district boundary polygons and extracting district/state labels from EM-DAT floods.

A) District boundary dataset acquired (GADM)
- Downloaded: gadm41_IND_shp.zip (GADM v4.1, India)
- Extracted to: 01_Data_Raw/District_Boundaries/
- Validated via geopandas load test:
  - gadm41_IND_2.shp loaded successfully
  - 676 district polygons
  - Key columns confirmed: NAME_1 (state), NAME_2 (district), geometry

B) EM-DAT district/state extraction completed
- Script created: 04_Code/06_parse_emdat_locations.py
- Outputs:
  - 02_Data_Intermediate/emdat_districts_parsed.csv
  - 05_Outputs/Logs/06_parse_emdat_log.txt
- Results:
  - Total flood events in file: 69
  - Events with usable Admin Units (adm2_name districts or fallback adm1_name states): 57 (82.6%)
  - Events missing Admin Units: 12 (17.4%) → parsed from free-text Location field
  - Geographic labels produced for all 69 events (districts and/or states)

C) Output verification
- Script created: 04_Code/07_check_output.py
- Verified:
  - emdat_districts_parsed.csv loads
  - 69/69 events have non-empty districts_final_str

Notes / Known issues
- Parsed Location text contains typos and non-district tokens in some cases (e.g., “Administrative unit not available”, “Itanagar distrci”, “Meghalaya states”).
- These will be handled during crosswalk cleaning (manual corrections + fuzzy match).

Status: District boundaries locked; EM-DAT geographic extraction complete.
Next: Build district name crosswalk (RBI ↔ GADM ↔ EM-DAT) and then generate flood exposure panel (Rule A vs Rule B).

======================================================================
2026-01-10 (Sat) 22:45-23:35 PM IST: Phase 3c Day 2.5 - Documentation Hygiene (No Coding)
======================================================================
Goal: Lock documentation language to avoid overclaiming and ensure repo docs match the actual folder structure and current phase status.

Actions Completed
A) Claim discipline updates (documentation-only)
- Core_Claims.docx: softened “first causal evidence” phrasing to “aims to provide” and made propagation wording explicitly conditional on data feasibility.
- Literature_Tracker.xlsx: softened top novelty claim from “establishes first causal evidence” to “aims to provide causal evidence consistent with.”

B) Identification / IV discipline wording (documentation-only)
- Hypotheses_Formal_v1.1.md: rewrote the H2b exclusion restriction paragraph to frame it as an identifying assumption with explicit threats (e.g., direct banking-operations disruption), without changing the empirical plan.

C) Codebook consistency (documentation-only)
- Variables_Codebook_and_Coding_Protocol_v1.1.md:
  - Updated the District Polygons status to reflect that GADM v4.1 Level-2 district boundaries are already downloaded and validated (676 districts).
  - Removed any stray [file:*] artifacts left from prior drafting.

D) README cleanup (documentation-only)
- README.md:
  - Removed [file:*] artifacts.
  - Updated repo layout to reflect current v1.1 filenames and current pipeline state.
  - Preserved RBI file extensions as .xlsx (local saved format).
  - Fixed markdown formatting issues (closed code fences) and removed duplicated outdated “Last Updated/Phase” footer lines.

Commits (documentation-only)
- Commit: “Tighten Core Claims wording”
- Commit: “Update novelty statement in Literature Tracker”
- Commit: “Clarify IV assumption language (Hypotheses v1.1)”
- Commit: “Update codebook: GADM obtained + cleanup”
- Commit: “README cleanup: consistency + formatting”

Status:
Documentation is now aligned with the current project state (Phase 3c Day 2 complete; crosswalk pending). No data or code changes were made in this session.

Next:
Phase 3c Day 3: Build district-name crosswalk (RBI ↔ GADM ↔ EM-DAT), generate district_crosswalk_draft.csv, and produce crosswalk log with match-rate stop condition check.

======================================================================
2026-01-11 (Sun) 20:10-21:00 PM IST: Phase 3c Day 3–Day 4 - Crosswalk + Flood Exposure Panel
======================================================================
Goal: Build a reproducible district-quarter flood exposure panel (Rule A/B) on a fixed geography, with logged match rates + validation checks.

Actions Completed
A) District name crosswalk (RBI ↔ GADM; EM-DAT ↔ GADM)
- Script executed: 04_Code/08_build_district_crosswalk.py
- Inputs:
  - GADM districts: 01_Data_Raw/District_Boundaries/gadm41_IND_2.shp (666 districts)
  - RBI districts: 01_Data_Raw/RBI_Bank_Data/RBI_Deposits_2023_2024.xlsx (762 districts)
  - EM-DAT parsed tokens: 02_Data_Intermediate/emdat_districts_parsed.csv (209 districts)
- Outputs:
  - 02_Data_Intermediate/district_crosswalk_draft.csv (769 rows)
  - 02_Data_Intermediate/emdat_district_matches.csv (209 rows)
  - 05_Outputs/Logs/08_build_crosswalk_log.txt
- Match rates (stop conditions):
  - RBI → GADM: 83.2% (threshold: 80%) → PASSED
  - EM-DAT → GADM: 81.3% (informational; threshold: 75%)

B) Panel skeleton construction (district × quarter)
- Script executed: 04_Code/09_build_quarterly_skeleton.py
- Output:
  - 02_Data_Intermediate/district_quarter_skeleton.csv
- Target structure: 2015Q1–2024Q4 (40 quarters), intended to be the backbone index for merges.

C) Flood exposure panel construction (Rule A vs Rule B)
- Script executed: 04_Code/10_build_flood_exposure.py
- Output:
  - 02_Data_Intermediate/flood_exposure_panel.csv
- Panel dimensions (as generated):
  - Total district-quarters: 26,640
  - Districts: 659
  - Quarters: 40
  - Date range: 2015Q1 to 2024Q4
- Exposure rates:
  - Rule A (full sample; state fallback allowed): 2,220 district-quarters (8.33%)
  - Rule B (high-precision; district-only): 272 district-quarters (1.02%)

D) Validation + summary logging
- Script executed: 04_Code/11_validate_flood_events.py
  - ✓ Event 2019-0331-IND: 33 Rule B districts correctly coded
  - ✓ Event 2023-0428-IND: State fallback triggered for HP, Delhi (Rule A only)
  - ✓ Event 2015-0504-IND: 9/10 AP/TN districts matched (90% rate)
- Script executed: 04_Code/12_summarize_flood_exposure.py
  - Output log: 05_Outputs/Logs/12_flood_exposure_summary.txt (exposure rates, top districts/states, limitations)

E) Design decision logged (geography standard)
- Logged decision: Use GADM districts as panel skeleton (not RBI districts)
- Output:
  - 05_Outputs/Logs/Phase3c_Day4_Design_Decision.txt
- Consequence recorded: RBI deposit data will be merged into GADM skeleton via district_crosswalk_draft.csv; unmatched RBI districts will be dropped or manually investigated in Phase 3d.

Known Issues / Limitations (as of Day 4)
- 46 unmatched EM-DAT location tokens remain (typos, J&K gaps, historical names).
- State-level fallback in Rule A introduces measurement error by construction.
- Historical district name changes (e.g., “Cuddapah” → “Kadapa”) are not fully resolved.

Status:
Crosswalk + quarterly skeleton + flood exposure panel are now constructed and logged (Rule A/B), with basic validation checks passed.

Next:
Phase 3d: Extract RBI deposits to long panel, bulk-download/aggregate VIIRS to districts, then merge RBI + VIIRS + flood exposure into a single district-quarter analysis dataset (03_Data_Clean), with missingness and drop logs.

======================================================================
2026-01-12 (Mon) 18:45-19:05 PM IST: Phase 3d - RBI Deposits Panel + Master Merge
======================================================================
Goal: Convert RBI deposits to a GADM district-quarter panel and merge deposits + floods into a single analysis-ready dataset.

Actions Completed
A) RBI deposits extraction (quarterly panel)
- Script executed: 04_Code/13_extract_rbi_deposits.py
- Outputs:
  - 02_Data_Intermediate/rbi_deposits_panel.csv
- Key fix: Aggregated to unique (district_gadm, state_gadm, quarter) to eliminate duplicate merge keys caused by ambiguous district-name crosswalk matches.

B) Master panel merge (skeleton + floods + deposits)
- Script executed: 04_Code/14_merge_master_panel.py
- Output:
  - 02_Data_Intermediate/master_panel_raw.csv
- Merge keys standardized to: (district_gadm, state_gadm, quarter)

C) Validation + missingness diagnostics
- Scripts executed:
  - 04_Code/15_validate_master_panel.py
  - 04_Code/16_diagnose_missing_data.py
- Output:
  - 02_Data_Intermediate/master_panel_validation_log.txt
- Findings:
  - Panel structure is balanced at 666 districts × 40 quarters = 26,640 rows.
  - Deposits blackout identified for 2016Q3–2017Q1 (2016Q3/Q4 and 2017Q1 fully missing).
  - 35 districts have 0% deposit coverage across the entire window (likely crosswalk/name-change issues).

D) Analysis sample construction (restricted sample)
- Script executed: 04_Code/17_prepare_analysis_sample.py
- Output:
  - 02_Data_Intermediate/master_panel_analysis.csv
- Restrictions applied:
  - Dropped quarters: 2016Q3, 2016Q4, 2017Q1
  - Dropped districts: 35 districts with zero deposit coverage
- Final analysis sample:
  - 631 districts × 37 quarters = 23,347 observations
  - Deposit coverage: 99.1%
  - Flood events (Rule A): 1,984 (all with deposits present in the restricted sample)

Status: Phase 3d master panel construction completed through analysis-sample output; VIIRS integration remains.
Next: Begin VIIRS monthly download/aggregation to district-quarter, then engineer regression variables (growth rates, lags) and generate descriptive tables.

======================================================================
END OF LOG - Last Updated: 2026-01-12 19:37 PM IST
======================================================================
